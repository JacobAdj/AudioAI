example to encode {'text': 'negen', 'audiofile': 'number9.wav'}
example to encode with audio {'text': 'negen', 'audiofile': 'number9.wav', 'audio_id': 9, 'language': 9, 'gender': 'female', 'speaker_id': '1122', 'is_gold_transcript': True, 'accent': 'None', 'audio': {'path': None, 'array': array([ 5.40012479e-13,  5.68434189e-14, -2.84217094e-13, ...,
        1.63729965e-06, -2.13144413e-05,  7.15546776e-07]), 'sampling_rate': 16000}}
2025-04-14 05:22:48.351861: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2025-04-14 05:22:48.352211: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
['input_ids', 'labels']

example after encoding {'input_ids': [4, 9, 5, 21, 5, 9, 2], 'labels': [[-6.107358932495117, -6.008665561676025, -6.188313961029053, -6.66566276550293, -6.2519612312316895, -6.15173864364624, -6.364901065826416, -6.817481517791748, -6.411930561065674, -6.304193496704102, -6.486514568328857, -6.9998931884765625, -6.944747447967529, -6.718105316162109, -6.7907633781433105, -6.997269153594971, -7.447695732116699, -7.2751288414001465, -6.932361602783203, -6.8636274337768555, -7.095805644989014, -7.112277030944824, -6.701137065887451, -6.630753517150879, -6.94347620010376, -6.838796615600586, -6.462897777557373, -6.483707427978516, -6.854288578033447, -6.647263050079346, -6.472410678863525, -6.7608323097229, -6.9555583000183105, -6.805910110473633, -7.2330241203308105, -7.150411128997803, -7.23112678527832, -7.034813404083252, -7.057388782501221, -6.872044086456299, -6.982051372528076, -6.806406021118164, -7.053550720214844, -7.047352313995361, -7.376465797424316, -6.895762920379639, -6.86027717590332, -6.559615135192871, -6.721533298492432, -6.668091773986816, -6.847299098968506, -7.139522075653076, -7.395966529846191, -7.222653388977051, -7.006040096282959, -6.909777641296387, -6.880789756774902, -6.758622646331787, -6.586626052856445, -6.528491973876953, -6.593347072601318, -6.647582054138184, -6.80794095993042, -7.078171253204346, -7.343366622924805, -7.67390251159668, -7.799257755279541, -7.140548229217529, -6.823756694793701, -6.723429203033447, -6.701487064361572, -6.8520827293396, -6.9330620765686035, -7.097409248352051, -7.420032501220703, -7.488470554351807, -7.429570198059082, -7.4484100341796875, 



Genereated nedzjen

type(inputs["input_ids"]) tensor([[ 4,  9,  5, 21,  5,  9,  2]])
spectrogram tensor([[-2.9900, -3.0863, -3.1134,  ..., -3.8667, -3.9236, -4.1241],
        [-2.7551, -2.8165, -2.8270,  ..., -3.7417, -3.7662, -4.0367],
        [-2.6076, -2.6062, -2.3987,  ..., -3.7187, -3.7933, -4.0534],
        ...,
        [-2.5494, -2.6121, -2.6268,  ..., -3.8915, -3.9845, -4.2370],
        [-2.6208, -2.6991, -2.7418,  ..., -3.9562, -4.0400, -4.2926],
        [-2.7156, -2.8197, -2.8836,  ..., -3.9937, -4.0651, -4.3066]])
spectrogram type <class 'torch.Tensor'>
speech tensor([-6.3614e-05,  1.3406e-04,  7.6830e-05,  ..., -5.2545e-06,
         2.2983e-04,  3.7980e-04])
Speech synthesis complete and saved



spectrogram = model.generate_speech(inputs["input_ids"], speaker_embeddings)

print('spectrogram' , spectrogram)
print('spectrogram type' , type(spectrogram))


spectrogram tensor([[-6.0412, -6.1761, -6.4942,  ..., -7.0709, -7.1176, -7.0899],
        [-4.9332, -5.0839, -5.1893,  ..., -5.5032, -5.5883, -5.5498],
        [-4.2606, -4.1858, -3.9879,  ..., -5.0671, -5.0776, -5.0122],
        ...,
        [-2.2465, -1.8825, -1.3598,  ..., -2.5507, -2.4896, -2.5137],
        [-2.3822, -1.9818, -1.4160,  ..., -2.3413, -2.1227, -2.1692],
        [-2.3254, -1.9471, -1.5034,  ..., -2.3502, -2.2113, -2.2419]])
spectrogram type <class 'torch.Tensor'>





training data negen [4, 9, 5, 21, 5, 9, 2]
training data negen labels type <class 'list'>
now training
  0%|                                                                                                                              | 0/1 [00:00<?, ?it/s]batch["input_ids"] tensor([[ 4, 14, 13, 10,  5,  2,  1],
        [ 4, 57,  5, 27,  5,  9,  2],
        [ 4,  6, 20,  5,  5,  2,  1],
        [ 4,  9,  5, 21,  5,  9,  2],
        [ 4, 27, 10, 46, 19,  2,  1],
        [ 4, 57,  5, 12,  2,  1,  1],
        [ 4, 73, 73,  9,  2,  1,  1],
        [ 4,  6, 10,  5,  9,  2,  1],
        [ 4, 27, 10,  5, 13,  2,  1],
        [ 4,  7, 17, 11,  6,  2,  1]])


now training
  0%|                                                                                                                              | 0/1 [00:00<?, ?it/s]batch["input_ids"] tensor([ 4,  9,  5, 21,  5,  9,  2])
batch["labels"] tensor([[-6.1074, -6.0087, -6.1883,  ..., -7.4484, -7.2576, -6.9311],
        [-5.0294, -5.1903, -5.0873,  ..., -5.5540, -5.3734, -5.3521],
        [-4.8915, -4.9950, -4.8474,  ..., -5.0159, -4.8972, -5.0113],
        ...,
        [-5.1112, -4.7958, -4.5895,  ..., -4.7877, -4.8294, -4.8857],
        [-4.6591, -4.7560, -4.8254,  ..., -4.7792, -4.7901, -4.8213],
        [-4.2022, -4.4469, -4.5497,  ..., -4.7818, -4.8186, -4.9265]])





{'loss': 0.2408, 'grad_norm': 0.2912675142288208, 'learning_rate': 8.425e-05, 'epoch': 916.0}                                                                           
{'loss': 0.2688, 'grad_norm': 0.27451545000076294, 'learning_rate': 8.400000000000001e-05, 'epoch': 916.25}                                                             
{'loss': 0.2527, 'grad_norm': 0.1934756189584732, 'learning_rate': 8.375e-05, 'epoch': 916.5}                                                                           
{'loss': 0.2484, 'grad_norm': 0.23148222267627716, 'learning_rate': 8.350000000000001e-05, 'epoch': 916.75}                                                             
{'loss': 0.2271, 'grad_norm': 0.29179418087005615, 'learning_rate': 8.325e-05, 'epoch': 917.0}                                                                          
 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 3668/4000 [5:49:28<24:39,  4.46s/it]New best loss: 0.2271 - Saving model...
{'loss': 0.3268, 'grad_norm': 0.22912803292274475, 'learning_rate': 8.300000000000001e-05, 'epoch': 917.25}
{'loss': 0.3055, 'grad_norm': 0.24552789330482483, 'learning_rate': 8.275e-05, 'epoch': 917.5}                                                                          
{'loss': 0.2693, 'grad_norm': 0.2269117534160614, 'learning_rate': 8.25e-05, 'epoch': 917.75}                                                                           
{'loss': 0.242, 'grad_norm': 0.42162466049194336, 'learning_rate': 8.225000000000001e-05, 'epoch': 918.0}                                                               
{'loss': 0.3063, 'grad_norm': 0.262197881937027, 'learning_rate': 8.2e-05, 'epoch': 918.25}                                                                             
{'loss': 0.2966, 'grad_norm': 0.2494259625673294, 'learning_rate': 8.175000000000001e-05, 'epoch': 918.5}                                                               
{'loss': 0.3481, 'grad_norm': 0.2630275785923004, 'learning_rate': 8.15e-05, 'epoch': 918.75}                                                                           
{'loss': 0.2563, 'grad_norm': 0.28741422295570374, 'learning_rate': 8.125000000000001e-05, 'epoch': 919.0}                                                              
{'loss': 0.3248, 'grad_norm': 0.30279043316841125, 'learning_rate': 8.1e-05, 'epoch': 919.25}                                                                           
{'loss': 0.2496, 'grad_norm': 0.21618473529815674, 'learning_rate': 8.075e-05, 'epoch': 919.5}                                 









